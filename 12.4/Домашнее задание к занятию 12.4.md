#  Домашнее задание к занятию "12.4 Развертывание кластера на собственных серверах, лекция 2"

##  Задание 1: Подготовить инвентарь kubespray

Буду создавать кластер в локальной сети. 1 Control Plane и 3 work nodes.

```
root@docker:~/git-repos/kubespray/inventory/mycluster# cat inventory.ini
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
node1 ansible_host=192.168.1.24  # ip=10.3.0.1 etcd_member_name=etcd1
node2 ansible_host=192.168.1.22  # ip=10.3.0.2 etcd_member_name=etcd2
node3 ansible_host=192.168.1.23  # ip=10.3.0.3 etcd_member_name=etcd3
node4 ansible_host=192.168.1.25  # ip=10.3.0.4 etcd_member_name=etcd4
# node5 ansible_host=95.54.0.16  # ip=10.3.0.5 etcd_member_name=etcd5
# node6 ansible_host=95.54.0.17  # ip=10.3.0.6 etcd_member_name=etcd6

# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
node1
# node2
# node3

[etcd]
node1
# node2
# node3

[kube_node]
node2
node3
node4
# node5
# node6

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```

```
ansible-playbook -i inventory/mycluster/inventory.ini --become --become-user=root cluster.yml
```
Получаем результат
![kubespray](kubespray_result.JPG)

Проверяем кластер
```
root@node1:~# kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

root@node1:~# kubectl get nodes
NAME    STATUS   ROLES                  AGE    VERSION
node1   Ready    control-plane,master   131m   v1.23.1
node2   Ready    <none>                 130m   v1.23.1
node3   Ready    <none>                 130m   v1.23.1
node4   Ready    <none>                 130m   v1.23.1
```

