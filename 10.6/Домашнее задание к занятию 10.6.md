# Домашнее задание к занятию "10.06. Инцидент-менеджмент"

## Задание 1

|Краткое описание инцидента| В 22:52 21 октября 2018 плановые работы, по техническому обслуживанию по замене вышедшего из строя оптического оборудования 100G, привели к потере связи между сетевым центром на восточном побережье США и основным центром обработки данных на восточном побережье США. Данная замена повлекла за собой отображение неактуальных данных и частичную неработоспособность сервисов на протежении 24 часов и 11 минут.|
| :-- | :-- |
| **Предшествующие события** | Замена вышедшего из строя оптического оборудования 100G |
| **Причина инцидента** | Потеря соединения на 100G оптическом канали связи между US East Coast network hub и primary US East Coast data center на 43 секунды, что привело к несогласованности кластеров MySQL. |
| **Воздействие** | Несогласованность предоставляемой информации -  отображение не актуальной информации. В большинстве случаев, не могли работать WebHook и GitHub Pages. |
| **Обнаружение** | Обнаружено инженерами, обрабатывающими входящие уведомления систем монторинга |
| **Реакция** | Восстановление работоспособности сервисов длилось 24 часа и 11 минут |
| **Восстановление** | Восстановление работоспособности было достигнуто за счет восстановления данных из бэкаповх и повторных репликаций данных |
| **Таймлайн** | **2018 October 21 22:52 UTC** - После восстановления сетевого оборудования, была начата репликация данных, но данные в БД различялись, что привело к несогласованности в рамках кластера. **2018 October 21 22:54 UTC** - Системы мониторинга начали генерировать предупреждения о неисправностях. **2018 October 21 23:02 UTC** - Инженеры определили, что топологии для многочисленных кластеров баз данных находятся в неожиданном состоянии. **2018 October 21 23:07 UTC** - Команда разработчиков решила вручную заблокировать внутренние инструменты развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. **2018 October 21 23:09 UTC** - Ответившая группа поместила сайт в желтый статус и было отправлено предупреждение координатору инцидента. **2018 October 21 23:11 UTC** - Координатор инцидента принял решение изменить статус на красный. **2018 October 21 23:13 UTC** - Были приглашены дополнительные инженеры из группы разработки баз данных GitHub. Выпонены действия для сохранения пользовательских данных. **2018 October 21 23:19 UTC** - Приостановлена доставка веб-перехватчиков и сборки GitHub Pages, чтобы не подвергать опасности данные. **2018 October 22 00:05 UTC** - Инженерами начата разработка плана устранения несоответствий данных. Восстановить данные из резервных копий, синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания и затем возобновить обработку заданий в очереди. Обновлен статус, чтобы сообщить пользователям о предстоящих работах. **2018 October 22 00:41 UTC** - Запущен процесс резервного копирования всех затронутых кластеров MySQL. **2018 October 22 06:51 UTC** - Несколько кластеров завершили восстановление из резервных копий в центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья. **2018 October 22 07:46 UTC** - Опубликована расширенная информация для пользователей. **2018 October 22 11:12 UTC** - Востановлены сервера в US East Coast, продолжается реплицирование. Налюдается повышенная нагрузка при реплицировании. **2018 October 22 13:15 UTC** - Приблизились к пиковому периоду нагрузок. Увеличено количество репликаций для снятии растущей нагрузки по реклицированию. **2018 October 22 16:24 UTC** - Реплики синхронизированы, переключение в штатную топологию. Сохранили статус службы красным. **2018 October 22 16:45 UTC** - После восстановления возникла необходимость балансировки нагрузки до восстановления 100% услуг клиентам. Для восстановления уже имеющихся данных пользователей включили обработку, внесли изменения в TTL до полного завершения и возвращения к штатной работе. **2018 October 22 23:03 UTC** - Все данные обработаны, была подтверждена целостность и правильная работа всех систем. Статус сайта был обновлен до зеленого.|
| **Последующие действия** | Собраны и проанализированы бинарные логи с Mysql серверов. Проводится анализ этих журналов и определяется, какие записи могут быть автоматически согласованы, а какие потребуют взаимодействия с пользователями. Сделан анализ предоставления информации. Ряд технических изменений - 1. Настройка конфигурации Orchestrator, чтобы предотвратить продвижение основных параметров базы данных через региональные границы. 2. Ускорили переход к новому механизму отчетности о статусе. 3. Начата общекорпоративная инженерная инициатива по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в режиме «active/active/active». Цель этой работы - выдержать полный отказ одного центра обработки данных без воздействия на пользователя. 4. Настройка большего кол-ва тестов. Так же улучшить практику проверки сценариев сбоев и внедрение chaos engineering.|


## Задача повышенной сложности


- 3.22 PM - db2 Kafka не работает, служба не работает 
- 3.23 PM - перезапуск не помог, откат на предыдущую версию не помог
- 3.24 PM - наблюдается много ошибок в логах Kafka, наблюдается сбой на приложениях андройд у клиентов и в веб приложении
- 3.27 PM - проблема может быть с кластером Mesos 
- 3.29 PM - нужно подготовить уведомления для клиентов и черновик опубликовать в канале Слак
- 3.30 PM - опубликовано уведомления для клиентов на на странице и Twitter 
- 3.36 PM - возможно нужно перезапустить Mesos 
- 3.39 PM - пока воздержимся от перезвпуска 
- 3.41 PM - mesos agent 2 и 3 недоступен 
- 3.43 PM - остался 1 агент кластера и память постоянно растет, Kafka то подымается, то падает 
- 3.45 PM - есть еще 1 кластерт Месос в другом регионе, и мы можем тогда перезарустить кластер Месос 
- 3.48 PM - Принято решение перезапустить агент 3 
- 3.49 PM - Linux OOM Killer сработал на агенте 1 
- 3.52 PM - Принято решение перезапустить агент 2 
- 3.53 PM - Агент 3 вернулся и работает, принято решение запуска контейнеров вручную на агенте 3 
- 3.55 PM - контейнеры не запускаются и выводят 137 ошибку 
- 3.57 PM - выяснили, это из-за Linux OOM Killer 
- 3.58 PM - выяснили предел памяти 130MB, решили вручную удвоить размер памяти, 
- 4.05 PM - это помогло и контейнеры запускаются без ошибок, консоль Maraton тоже показывает что все выглядит хорошо. Мобильные приложения клинтов работаю, но пока есть проблема в веб интерфейсе 
- 4.06 PM - отстают данные из-за простоя Kafka
- 4.07 PM - нужно подождать данных, которые отстали. Составить черновик сообщения для клиентов 
- 4.08 PM - опубликовать на странице и Twitter сообщение для клиентов. Всё выглядит хорошо, идет восстановление системы
- 4.09 PM - добавили список задач, для предотвращения данных инцидентов