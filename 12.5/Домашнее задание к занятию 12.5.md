#  Домашнее задание к занятию "12.5 Сетевые решения CNI"

##  Задание 1: установить в кластер CNI плагин Calico

- Проверяем что у нас установлено
```
root@node1:~# kubectl get nodes,pod,svc --all-namespaces
root@node1:~/pods# kubectl get nodes,pod,svc --all-namespaces
NAME         STATUS   ROLES                  AGE    VERSION
node/node1   Ready    control-plane,master   4d3h   v1.23.1
node/node4   Ready    <none>                 4d3h   v1.23.1

NAMESPACE     NAME                                           READY   STATUS    RESTARTS     AGE
default       pod/backend-f785447b9-krrzg                    1/1     Running   0            3h13m
default       pod/frontend-8645d9cb9c-7k2z2                  1/1     Running   0            3h52m
default       pod/hello-node-6b89d599b9-tzkcd                1/1     Running   0            4h12m
dev           pod/dev-web1-5f45c88676-vl6hm                  1/1     Running   0            98m
dev           pod/dev-web2-5cd468cdf5-sjnmb                  1/1     Running   0            96m
kube-system   pod/calico-kube-controllers-7c4d5b7bf4-rpw84   1/1     Running   0            8h
kube-system   pod/calico-node-bq2gt                          1/1     Running   1 (8h ago)   4d3h
kube-system   pod/calico-node-cb29j                          1/1     Running   1 (8h ago)   4d3h
kube-system   pod/coredns-76b4fb4578-7fwj2                   1/1     Running   1 (8h ago)   4d3h
kube-system   pod/coredns-76b4fb4578-b6pvq                   1/1     Running   0            8h
kube-system   pod/dns-autoscaler-7979fb6659-fckwf            1/1     Running   1 (8h ago)   4d3h
kube-system   pod/kube-apiserver-node1                       1/1     Running   2 (8h ago)   4d3h
kube-system   pod/kube-controller-manager-node1              1/1     Running   4 (8h ago)   4d3h
kube-system   pod/kube-proxy-gqth9                           1/1     Running   1 (8h ago)   4d3h
kube-system   pod/kube-proxy-nj6gx                           1/1     Running   1 (8h ago)   4d3h
kube-system   pod/kube-scheduler-node1                       1/1     Running   3 (8h ago)   4d3h
kube-system   pod/nginx-proxy-node4                          1/1     Running   1 (8h ago)   4d3h
kube-system   pod/nodelocaldns-mqstd                         1/1     Running   2 (8h ago)   4d3h
kube-system   pod/nodelocaldns-nnstv                         1/1     Running   1 (8h ago)   4d3h

NAMESPACE     NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       service/backend      ClusterIP   10.233.59.85    <none>        80/TCP                   3h13m
default       service/frontend     ClusterIP   10.233.63.137   <none>        80/TCP                   3h52m
default       service/hello-node   ClusterIP   10.233.60.6     <none>        8080/TCP                 162m
default       service/kubernetes   ClusterIP   10.233.0.1      <none>        443/TCP                  4d3h
dev           service/dev-web1     ClusterIP   10.233.52.35    <none>        80/TCP                   98m
dev           service/dev-web2     ClusterIP   10.233.18.31    <none>        443/TCP                  96m
kube-system   service/coredns      ClusterIP   10.233.0.3      <none>        53/UDP,53/TCP,9153/TCP   4d3h
```

- Проверяем доступы
```
root@node1:~/pods# kubectl exec backend-f785447b9-krrzg -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.4
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node:8080
user-agent=curl/7.79.1
BODY:
-no body in request-

root@node1:~/pods# kubectl exec frontend-8645d9cb9c-7k2z2 -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.3
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node:8080
user-agent=curl/7.79.1
BODY:
-no body in request-

root@node1:~/pods# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 frontend
Praqma Network MultiTool (with NGINX) - frontend-8645d9cb9c-7k2z2 - 10.233.105.3

root@node1:~/pods# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 backend
Praqma Network MultiTool (with NGINX) - backend-f785447b9-krrzg - 10.233.105.4
```
```
root@node1:~/pods# kubectl -n dev exec dev-web1-5f45c88676-vl6hm -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.5
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node:8080
user-agent=curl/7.79.1
BODY:
-no body in request-
```
```
root@node1:~/pods# kubectl -n dev exec dev-web2-5cd468cdf5-sjnmb -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.6
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node:8080
user-agent=curl/7.79.1
BODY:
-no body in request-
```
```
root@node1:~/pods# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 dev-web1.dev
Praqma Network MultiTool (with NGINX) - dev-web1-5f45c88676-vl6hm - 10.233.105.5
root@node1:~/pods# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 dev-web2.dev:443
<html>
<head><title>400 The plain HTTP request was sent to HTTPS port</title></head>
<body>
<center><h1>400 Bad Request</h1></center>
<center>The plain HTTP request was sent to HTTPS port</center>
<hr><center>nginx/1.18.0</center>
</body>
</html>
```


- Доступ из всех нэймспесов к hello-node и обратно есть.

- Создадим политику по умолчанию, которая запрещает входящие запросы на все поды в default namespace.
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

- Проверяем
```
root@node1:~/policies# kubectl exec backend-f785447b9-krrzg -- curl -s -m 1 hello-node:8080
command terminated with exit code 28
```
```
root@node1:~/policies# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 dev-web1.dev
Praqma Network MultiTool (with NGINX) - dev-web1-5f45c88676-vl6hm - 10.233.105.5
```

```
root@node1:~/policies# kubectl exec hello-node-6b89d599b9-tzkcd -- curl -s -m 1 google.com
<HTML><HEAD><meta http-equiv="content-type" content="text/html;charset=utf-8">
<TITLE>301 Moved</TITLE></HEAD><BODY>
<H1>301 Moved</H1>
The document has moved
<A HREF="http://www.google.com/">here</A>.
</BODY></HTML>
```

- Как видим, доступы с default namespace есть в интернет и dev namespace, в сами поды default namespace доступа нет. 

- Создадим политику, которая разрешает доступ с frontend к hello-node

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: hello-frontend-access
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: hello-node
  policyTypes:
    - Ingress
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: frontend
      ports:
        - protocol: TCP
          port: 8080
```
- Проверяем

```
root@node1:~/policies# kubectl exec frontend-8645d9cb9c-7k2z2 -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.3
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node:8080
user-agent=curl/7.79.1
BODY:
-no body in request-
```

```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- curl -s -m 1 hello-node:8080
command terminated with exit code 28
```

- Создадим политику, которая разрешает доступ с namespace dev и только из dev-web1 пода к hello-node

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: hello-web1-ingress
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: hello-node
  policyTypes:
    - Ingress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: dev
            stage: dev
        podSelector:
          matchLabels:
            app: dev-web1
      ports:
        - protocol: TCP
          port: 8080
```
- Проверяем
```
root@node1:~/policies# kubectl exec -n dev dev-web1-5f45c88676-vl6hm  -- curl -s -m 1 hello-node:8080
CLIENT VALUES:
client_address=10.233.105.5
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://hello-node.default:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=hello-node.default:8080
user-agent=curl/7.79.1
BODY:
-no body in request-
```
```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- curl -s -m 1 hello-node:8080
command terminated with exit code 28
```

- Создадим политику, которая позволяет поду dev-web2 из namespace dev обращаться только на внутренние ресурсы, без выхода в интернет.

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: dev-web2-egress-local
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: dev-web2
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/8
    - ipBlock:
        cidr: 192.168.0.0/16
    - ipBlock:
        cidr: 172.16.0.0/12
    - ipBlock:
        cidr: 169.254.0.0/16
    - ipBlock:
        cidr: 127.0.0.0/8
```
- Проверяем
```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- curl -s -m 1 dev-web1
Praqma Network MultiTool (with NGINX) - dev-web1-5f45c88676-vl6hm - 10.233.105.5
```
```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- ping -w 5 192.168.1.24
PING 192.168.1.24 (192.168.1.24) 56(84) bytes of data.
64 bytes from 192.168.1.24: icmp_seq=1 ttl=63 time=2.19 ms
64 bytes from 192.168.1.24: icmp_seq=2 ttl=63 time=0.272 ms
64 bytes from 192.168.1.24: icmp_seq=3 ttl=63 time=0.288 ms
64 bytes from 192.168.1.24: icmp_seq=4 ttl=63 time=0.378 ms
64 bytes from 192.168.1.24: icmp_seq=5 ttl=63 time=0.339 ms

--- 192.168.1.24 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4072ms
rtt min/avg/max/mdev = 0.272/0.693/2.192/0.750 ms
```
```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- curl -s -m 1 google.com
command terminated with exit code 28
```
```
root@node1:~/policies# kubectl exec -n dev dev-web2-5cd468cdf5-sjnmb  -- ping -w 5 ya.ru
PING ya.ru (87.250.250.242) 56(84) bytes of data.

--- ya.ru ping statistics ---
5 packets transmitted, 0 received, 100% packet loss, time 4052ms

command terminated with exit code 1
```
- Как видим, все правила работают как надо.

- Вот список политик
```
root@node1:~/policies# kubectl get networkpolicies -A
NAMESPACE   NAME                    POD-SELECTOR     AGE
default     default-deny-ingress    <none>           2d18h
default     hello-frontend-access   app=hello-node   2d17h
default     hello-web1-ingress      app=hello-node   21h
dev         dev-web2-egress-local   app=dev-web2     18h
```

##  Задание 2: изучить, что запущено по умолчанию

```
root@node1:~/policies# calicoctl get nodes
NAME
node1
node4
```
```
root@node1:~/policies# calicoctl get ipPool
NAME           CIDR             SELECTOR
default-pool   10.233.64.0/18   all()
```
```
root@node1:~/policies# calicoctl get profile
NAME
projectcalico-default-allow
kns.default
kns.dev
kns.kube-node-lease
kns.kube-public
kns.kube-system
ksa.default.default
ksa.dev.default
ksa.kube-node-lease.default
ksa.kube-public.default
ksa.kube-system.attachdetach-controller
ksa.kube-system.bootstrap-signer
ksa.kube-system.calico-kube-controllers
ksa.kube-system.calico-node
ksa.kube-system.certificate-controller
ksa.kube-system.clusterrole-aggregation-controller
ksa.kube-system.coredns
ksa.kube-system.cronjob-controller
ksa.kube-system.daemon-set-controller
ksa.kube-system.default
ksa.kube-system.deployment-controller
ksa.kube-system.disruption-controller
ksa.kube-system.dns-autoscaler
ksa.kube-system.endpoint-controller
ksa.kube-system.endpointslice-controller
ksa.kube-system.endpointslicemirroring-controller
ksa.kube-system.ephemeral-volume-controller
ksa.kube-system.expand-controller
ksa.kube-system.generic-garbage-collector
ksa.kube-system.horizontal-pod-autoscaler
ksa.kube-system.job-controller
ksa.kube-system.kube-proxy
ksa.kube-system.namespace-controller
ksa.kube-system.node-controller
ksa.kube-system.nodelocaldns
ksa.kube-system.persistent-volume-binder
ksa.kube-system.pod-garbage-collector
ksa.kube-system.pv-protection-controller
ksa.kube-system.pvc-protection-controller
ksa.kube-system.replicaset-controller
ksa.kube-system.replication-controller
ksa.kube-system.resourcequota-controller
ksa.kube-system.root-ca-cert-publisher
ksa.kube-system.service-account-controller
ksa.kube-system.service-controller
ksa.kube-system.statefulset-controller
ksa.kube-system.token-cleaner
ksa.kube-system.ttl-after-finished-controller
ksa.kube-system.ttl-controller
```

